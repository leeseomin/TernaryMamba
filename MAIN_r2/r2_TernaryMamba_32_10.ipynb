{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482a1f7-5365-486c-b878-951ee971e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm mamba_ssm matplotlib safetensors tiktoken datasets\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from mamba_ssm import Mamba\n",
    "import matplotlib.pyplot as plt\n",
    "from safetensors.torch import save_file\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "os.makedirs('/content/drive/MyDrive/Colab Notebooks', exist_ok=True)\n",
    "\n",
    "# Hyperparams\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "block_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_iters = 7000\n",
    "print_iters = 100\n",
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "n_embed = 384\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "num_proc_load_dataset = 8\n",
    "expansion_factor = 4\n",
    "Qb = 1\n",
    "\n",
    "# Load and process data\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "    ids.append(enc.eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = load_dataset(\"openwebtext\", split='train[:10%]', num_proc=num_proc_load_dataset)\n",
    "    split_dataset = dataset.train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "    split_dataset['val'] = split_dataset.pop('test')\n",
    "    tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=num_proc_load_dataset,\n",
    "    )\n",
    "    \n",
    "    data_dir = '/content/drive/MyDrive/Colab Notebooks/data'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = os.path.join(data_dir, f'{split}.bin')\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        idx = 0\n",
    "        for example in tqdm(dset, desc=f'writing {filename}'):\n",
    "            arr[idx : idx + example['len']] = example['ids']\n",
    "            idx += example['len']\n",
    "        arr.flush()\n",
    "    \n",
    "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "    # Convert uint16 to int64\n",
    "    train_data = train_data.astype(np.int64)\n",
    "    val_data = val_data.astype(np.int64)\n",
    "\n",
    "    train_data = torch.tensor(train_data, dtype=torch.long)\n",
    "    val_data = torch.tensor(val_data, dtype=torch.long)\n",
    "\n",
    "# Quantization function\n",
    "def quantize_weights(weights):\n",
    "    abs_mean = torch.mean(torch.abs(weights))\n",
    "    scaled_weights = weights / (abs_mean + 1e-8)\n",
    "    return torch.round(torch.clamp(scaled_weights, -1, 1))\n",
    "\n",
    "# Mamba Block\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor, SSM_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.swiglue = nn.Sequential(\n",
    "            nn.Linear(d_model, expansion_factor*d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(expansion_factor*d_model, d_model)\n",
    "        )\n",
    "        self.swiglue[0].weight.data = quantize_weights(self.swiglue[0].weight.data)\n",
    "        self.swiglue[2].weight.data = quantize_weights(self.swiglue[2].weight.data)\n",
    "        self.SSM = Mamba(d_model=d_model, d_state=SSM_dim, d_conv=4, expand=expansion_factor).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.swiglue(self.ln1(x))\n",
    "        x = x + self.SSM(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Language Model\n",
    "class MambaLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, expansion_factor, SSM_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.embed.weight.data = quantize_weights(self.embed.weight.data)\n",
    "        self.layers = nn.ModuleList([MambaBlock(d_model, expansion_factor, SSM_dim)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.ln_out = nn.LayerNorm(d_model)\n",
    "        self.proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.proj.weight.data = quantize_weights(self.proj.weight.data)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Scale activations to [-Qb, Qb] per token\n",
    "        x_abs_max = torch.amax(torch.abs(x), dim=-1, keepdim=True)\n",
    "        x_scale = Qb / (x_abs_max + 1e-8)\n",
    "        x = x * x_scale\n",
    "\n",
    "        x = self.ln_out(x)\n",
    "        logits = self.proj(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# Training and evaluation\n",
    "vocab_size = enc.max_token_value + 1\n",
    "model = MambaLM(vocab_size, n_embed, n_layers, expansion_factor=expansion_factor, SSM_dim=16).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        _, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Initialize the figure and axes for plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch(train_data)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        train_loss = estimate_loss(model, train_data, eval_iters)\n",
    "        val_loss = estimate_loss(model, val_data, eval_iters)\n",
    "        print(f\"Step {iter} | train loss {train_loss:.4f} | val loss {val_loss:.4f}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        ax.clear()\n",
    "        ax.plot(train_losses, label='Train Loss')\n",
    "        ax.plot(val_losses, label='Val Loss')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training and Validation Loss')\n",
    "        ax.legend()\n",
    "\n",
    "    if iter % print_iters == 0:\n",
    "        model.eval()\n",
    "        sample_idx = model.generate(torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=200)\n",
    "        print(enc.decode(sample_idx[0].tolist()))\n",
    "        model.train()\n",
    "\n",
    "    # Save model every 5000 iterations\n",
    "    if iter % 5000 == 0 and iter != 0:\n",
    "        save_file(model.state_dict(), f\"/content/drive/MyDrive/Colab Notebooks/mamba_model_iter_{iter}.safetensors\")\n",
    "\n",
    "# Generate text\n",
    "model.eval()\n",
    "sample_idx = model.generate(torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)\n",
    "print(enc.decode(sample_idx[0].tolist()))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss')\n",
    "ax.plot(val_losses, label='Val Loss')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
